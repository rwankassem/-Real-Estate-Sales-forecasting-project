# -*- coding: utf-8 -*-
"""Copy_of_Sales_Forecating_project (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nHfcvXA2Tr6PHpwpRxpfwNM1Su8KTNWL

---


## **IMPORT LIBRARIES**
---
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""







---


## **LOADING DATASET**
---


"""

df=pd.read_csv('DMV_Homes.csv')
df.head()

df.info()

"""







---


## **DATA INSPECTION**
---


"""

df.shape

df.info()

df.describe()

cat_features = df.select_dtypes(include='object').columns.values
num_features = df.select_dtypes(exclude='object').columns.values

print("Categorical Features: \n\n", cat_features)
print("\n","="*80)
print("Numerical Features: \n\n", num_features)

df.describe(exclude='number').T

"""---

**OBSERVATION**


---


*   **Dataset Dimensions**: 5,493 rows × 10 columns


*  **Missing Values**:
--All columns have missing entries (5,484–5,486 present)
--Worst in: Year Built (12.2% missing), Square Footage (6.1%), Bedrooms (4.4%)



* **Suspicious Numerical Values**:
--Year Built: 0 (invalid) and 9999 (placeholder)
--Bathrooms: Max 76 (unrealistic)
--Bedrooms: Max 22 (unrealistic)
--Square Footage: 0 and 2M sq ft (outliers)

*   **Categorical Features:**

     City, State, Date Added, Status Change Date

---


## **DATA CLEANING**
---
"""

df.isnull().sum()

numeric_cols = ['Year Built', 'Bathrooms', 'Bedrooms', 'Square Footage']
for col in numeric_cols:
    median_val = df[col].median()
    df[col].fillna(median_val, inplace=True)

df.info()

zip_mode = df['Zip'].mode()[0]
df['Zip'].fillna(zip_mode, inplace=True)

object_cols = ['State', 'Date Added', 'Status Change Date']
for col in object_cols:
    col_mode=df[col].mode()[0]
    df[col].fillna(col_mode, inplace=True)

df.isnull().sum()

df = df.dropna(subset=['List Price'])

df.isnull().sum()

"""

---


**OBSERVATION**

---
Before handling missing values, several important columns (Year Built,Bathrooms, Bedrooms, Square Footage, List Price, City, State, Date Added, Status Change Date) contained missing entries.



*  For numeric columns, missing values were filled using --> median value of each column.

* The missing values in the Zip column were filled using the mode (most frequent zip code).

*  For categorical (text) columns, missing values were filled using --> mode (most frequent value) of each column.

"""

df=df.drop_duplicates()
df.duplicated().sum()

"""---
**OBSERVATION**


---

 there were 507 duplicate rows in the dataset.

After removing the duplicate rows using the drop_duplicates() function, there are now 0 duplicate rows remaining in the dataset.

---


## **DATA PREPROCESSING**
---

**Observation**

After INSPECTION & CLEANING Phases:


*   City feature has 77 variables
*   This could hurt the model and the preprocessing
*   City column does't fit the regular encoding method although it's signficant in the Visualization phase
*  Applying  Feature engineering to extract a new feature for the trainning
*   use frequency encoding or Target encoding and creat a new column "City_freq" or "city_price_mean"
*   after consideration and Experimentaions --->
    Target encoding (city_price_mean) method could create a target lekeage so decided to apply frequency encoding
"""

#Frequency encoding for city column
df['City_freq'] = df['City'].value_counts()
# Define city_freq before using it
city_freq = df['City'].value_counts().to_dict()
df['City_freq'] = df['City'].apply(lambda x: city_freq.get(x, 0) if pd.notna(x) else 0)

# Create a copy of the original DataFrame
df_encoded = df.copy()

# Import LabelEncoder
from sklearn.preprocessing import LabelEncoder

# Encode 'House Type' using LabelEncoder
le_house = LabelEncoder()
df_encoded['House Type'] = le_house.fit_transform(df_encoded['House Type'])

# Encode 'State' using LabelEncoder
le_state = LabelEncoder()
df_encoded['State'] = le_state.fit_transform(df_encoded['State'])

#seperation
# Exclude 'City' (case-sensitive) from Training columns
x=df.drop(['List Price','City'],axis=1)
y=df['List Price']

#ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)


numeric_cols = x_train.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = x_train.select_dtypes(include='object').columns

from sklearn.preprocessing import OrdinalEncoder
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_cols)
    ]
)
x_train_transformed = preprocessor.fit_transform(x_train)

# Extract encoded feature names
encoded_feature_names = preprocessor.get_feature_names_out()

# Convert to DataFrame for plotting and analysis
df_encoded = pd.DataFrame(x_train_transformed, columns=encoded_feature_names)

# Optionally add back the target column
df_encoded['price'] = y_train.reset_index(drop=True)

# Convert float columns to integers
df['Bathrooms'] = df['Bathrooms'].astype(int)
df['Bedrooms'] = df['Bedrooms'].astype(int)
df['Year Built']= df['Year Built'].astype(int)
df['City_freq']= df['City_freq'].astype(int)

df.drop('Date Added', axis=1, inplace=True)
df.drop('Status Change Date', axis=1, inplace=True)
df.drop('Zip', axis=1, inplace=True)

"""---
**OBSERVATION**


---
* Converting Columns to Integer Type:

The columns Bathrooms, Bedrooms, Year Built, and Zip were successfully converted to integer type (int).
*  Converting Date Columns to datetime Type:

The columns Date Added and Status Change Date were successfully converted to datetime format.

*   Calculating the House's Age:
"The 'House Age' column was successfully created by subtracting the 'Year Built' from the current year. This allows us to easily analyze the age distribution of the houses."





"""

df.head()

df.info()

df = df[df["List Price"] < df["List Price"].quantile(0.95)]

df=df[df['List Price'] <1525000]

import plotly.express as px
px.box(x=df['List Price'])

"""**OUTLIERS**"""

# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import RobustScaler, OneHotEncoder

# #object_cols = x.select_dtypes('O').columns.drop('City')  # Changed 'city' to 'City'

# # Add 'City_freq' to numeric columns if not already present
# numeric_cols = x.select_dtypes(exclude='O').columns.tolist()
# if 'City_freq' not in numeric_cols:
#     numeric_cols.append('City_freq')

# # Define preprocessor
# preprocessor = ColumnTransformer(
#     transformers=[
#         ('num', RobustScaler(), numeric_cols),
#         ('cat', OneHotEncoder(handle_unknown='ignore'), object_cols)
#     ]
# )

#EXPLORING OUTLIERS (NUMERIC COLUMNS )
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
fig, axes = plt.subplots(1, len(numeric_cols), figsize=(12,6))

for i,col in enumerate(numeric_cols) :
  sns.boxplot(data=df[col],ax=axes[i],color='skyblue')
  axes[i].set_title(f'{col}')

plt.suptitle('Box plots for numerical columns')
plt.tight_layout()
plt.show()

"""---


**All columns have Outlires --> all numerical missing values will be filled with median**

---

---


**OBSERVATION**

---

---
-Year Built has extreme outliers — some properties have illogical construction years (like 0 or values near 10,000).

-Bathrooms shows very high outliers — some listings have an unusually large number of bathrooms (more than 10–20).

-Bedrooms also contains outliers — properties with a very high number of bedrooms (over 15).

-Square Footage has significant outliers — some properties are extremely large (over 2 million sq ft).

-List Price contains extreme outliers — a few properties have list prices reaching up to 40 million dollars.

-House Age has major issues — negative house ages indicate errors, likely because the Status Change Date is earlier than the Year Built, which is not logically possible.
"""

# #using k-means
# from sklearn.cluster import KMeans

# kmeans = KMeans(n_clusters=3, random_state=42)
# df['Cluster'] = kmeans.fit_predict(df[['List Price', 'Square Footage']])

# sns.scatterplot(data=df, x='List Price', y='Square Footage', hue='Cluster')
# plt.show()

# from sklearn.preprocessing import StandardScaler

# scaler = StandardScaler()
# scaled_features = scaler.fit_transform(df[['List Price', 'Square Footage']])

# # Apply KMeans again
# kmeans_scaled = KMeans(n_clusters=3, random_state=42)
# df['Cluster_scaled'] = kmeans_scaled.fit_predict(scaled_features)

# # Visualize
# sns.scatterplot(x=scaled_features[:,0], y=scaled_features[:,1], hue=df['Cluster_scaled'])
# plt.xlabel('Scaled List Price')
# plt.ylabel('Scaled Square Footage')
# plt.title('KMeans Clustering after Scaling')
# plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt


# Standardizing the data (important for DBSCAN)
data = df[['Square Footage', 'List Price']].values  # Extracting the relevant columns (Square Footage and List Price)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)  # (mean = 0, standard deviation = 1)

# Applying DBSCAN
dbscan = DBSCAN(eps=.5, min_samples=5)  #  initialization with parameters
y_db = dbscan.fit_predict(data_scaled)

plt.figure(figsize=(8, 6))

plt.scatter(data_scaled[:, 0], data_scaled[:, 1], c=y_db, cmap='viridis', marker='o', edgecolor='k')

# Plotting the noise points (label == -1)
plt.scatter(data_scaled[y_db == -1, 0], data_scaled[y_db == -1, 1], color='red', marker='x', label='Noise')

plt.title('DBSCAN Clustering on Square Footage and List Price')
plt.xlabel('Standardized Square Footage')
plt.ylabel('Standardized List Price')
plt.legend()
plt.show()

# Loop to apply capping for each numerical column
for col in numeric_cols:
    # Calculate 1st and 99th percentiles
    lower_limit = df[col].quantile(0.01)
    upper_limit = df[col].quantile(0.99)

    # Apply clipping
    df[col] = df[col].clip(lower=lower_limit, upper=upper_limit)

print("Capping has been applied successfully to the numerical columns!")

#After scaling
fig, axes = plt.subplots(1, len(numeric_cols), figsize=(12,6))

for i,col in enumerate(numeric_cols) :
  sns.boxplot(data=df[col],ax=axes[i],color='skyblue')
  axes[i].set_title(f'{col}')

plt.suptitle('Box plots for numerical columns after scaling')
plt.tight_layout()
plt.show()

#EXPLORING OUTLIERS (CATEGORICAL COLUMNS )
fig, ax = plt.subplots(figsize=(12,6)) # Create a single subplot

for i, col in enumerate(df.select_dtypes(include='object').columns):
  sns.boxplot(data=df[col], ax=ax, color='skyblue')
  ax.set_title(f'{col}')  # Set title using ax

plt.suptitle('Box plots for categorical columns')
plt.tight_layout()
plt.show()

#After Scaling

fig, axes = plt.subplots(1, len(object_cols), figsize=(12,6))

# Updated loop to iterate through columns that actually exist in df
for i, col in enumerate(df.select_dtypes(include='object').columns):
    sns.boxplot(data=df[col], ax=axes[i], color='skyblue')
    axes[i].set_title(f'{col}')

plt.suptitle('Box plots for categorical columns after scaling')
plt.tight_layout()
plt.show()

df.head()

from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
import xgboost as xg
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

"""**SPLITTING**


---


"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Fit the preprocessor on the training data
preprocessor.fit(x_train)

# Transform the training data using the fitted preprocessor
x_train_transformed = preprocessor.transform(x_train)

# Transform the testing data
x_test_transformed = preprocessor.transform(x_test)

print(x_train_transformed.shape[0])

print(f"Before encoding: {x_train.shape}")
print(f"After encoding: {x_train_transformed.shape}")

numeric_cols = x_train.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = x_train.select_dtypes(include='object').columns

x_train.info()

x_train_scaled = preprocessor.fit_transform(x_train)
x_test_scaled = preprocessor.transform(x_test)

from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(score_func=f_regression, k=100)
x_train_selected = selector.fit_transform(x_train_transformed, y_train)
x_test_selected = selector.transform(x_test_transformed)

num_features = numeric_cols
# Get feature names used during fitting
categorical_cols = preprocessor.named_transformers_['cat'].feature_names_in_

# Now get encoded feature names
cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)

# Combine numeric and categorical feature names
all_features = list(num_features) + list(cat_features)

# Convert to DataFrame, ensuring the number of columns matches all_features
x_train_scaled_df = pd.DataFrame(x_train_scaled, columns=all_features, index=x_train.index)
x_test_scaled_df = pd.DataFrame(x_test_scaled, columns=all_features, index=x_test.index)

y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

x_train_scaled_df.info()

numeric_cols = x_train.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = x_train.select_dtypes(include='object').columns

x_train_scaled_df

"""---


**OBSERVATION**

---




*  Only numerical features were selected for scaling to prevent errors.
*  (StandardScaler) was applied to standardize the numerical data (mean = 0, standard deviation = 1).

*  After scaling, the data was reconstructed while keeping the original feature names and indices.

**FEATURE ENGINEERING**
---
"""

#FEATURE ENGINEERING
# add new feature to calculate the house's age
from datetime import date
current_year = date.today().year
df['House Age'] = current_year - df['Year Built']
print(df['House Age'])

#FEATURE ENGINEERING
# add new feature to categorize areas ( Large - Medium - Small )
q1=df['Square Footage'].quantile(0.25)
q3=df['Square Footage'].quantile(0.75)
bins=[df['Square Footage'].min(),q1,q3,df['Square Footage'].max()]

df['House Size'] = pd.cut(df['Square Footage'], bins=bins, labels=['Small','Medium','Large'])

# append new column(Size) to object_cols
object_cols.append('House Size')

print(df['House Size'].isnull().sum())

House_Size_mode = df['House Size'].mode()[0]
df['House Size'].fillna(House_Size_mode , inplace=True)

df.head()

#PCA
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


# Selecting features for PCA
features = ['Square Footage', 'List Price', 'Bathrooms', 'Bedrooms', 'House Age']  # Example features
X = df[features]

# Step 2: Standardize the data (PCA is sensitive to the scale of the data)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Apply PCA
pca = PCA(n_components=2)  # Reduce the data to 2 components for visualization
X_pca = pca.fit_transform(X_scaled)

# Step 4: Create a DataFrame with the PCA components
df_pca = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])

plt.figure(figsize=(8, 6))
plt.scatter(df_pca['PCA1'], df_pca['PCA2'], c=df['List Price'], cmap='viridis')
plt.colorbar(label='List Price')
plt.title('PCA ')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.show()

print("Explained variance ratio:", pca.explained_variance_ratio_)

"""




---


## **EDA FOR NUMERICAL FEATURES**
---"""

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
for col in numeric_cols:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[col],kde=True, bins=5,color='skyblue')
    plt.xlabel(col)
    plt.title(f' {col}')
    plt.show()

"""**Relation Between Numerical Features and The Target(The Price)**

"""

numerical_cols = ['Square Footage', 'Bathrooms', 'Bedrooms', 'Year Built']
for col in numerical_cols:
    plt.figure(figsize=(16, 8))
    sns.scatterplot(data=df, x=col, y='List Price', color='skyblue')
    plt.xlabel(col)
    plt.ylabel('List Price')
    plt.title(f' {col} vs. List Price')
    plt.show()

"""**Observation**


*   Square Footage vs List Price: has a positive correlation
*   Bathrooms and Bedrooms vs List Price: has a strong positive correlation
*   Year Built vs List Price: Not a Strong relation although the newest house has a tight price variance not as th older ones.

---


**OBSERVATION**

----

*   Calculating House size :

"The 'House Size ' column was successfully created by using 'pd.cut'. This allows us to easily analyze the size of the houses."

---


## **EDA FOR CATEGORICAL FEATURES**
---
"""

cols_to_remove = ['Date Added', 'Status Change Date']

# Ensure categorical_cols is updated to reflect dropped columns
categorical_cols = [col for col in categorical_cols if col not in cols_to_remove]

# Now proceed with your countplots
for col in categorical_cols:
    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x=col)
    plt.xlabel(col)
    plt.title(f' {col}')
    plt.show()

df = df[~df['State'].isin(['mode','WA','AL','MS'])]

df['State'] = df['State'].replace('Maryland', 'MD')

df.head()

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='State')
plt.title('Number of Properties by State')
plt.show()

"""---


**OBSERVATION**

---

*   After applying Visualization methods as Histogram and CountPlot. noticed that there is 2 different names for the same  Variable in State Column.
*   Used replace function to combine them under one name.
*   deleted unnecessary variables that are out of our states range

**Statistical Analysis**
"""

import pandas as pd
import numpy as np
import scipy.stats as ss
import seaborn as sns
import matplotlib.pyplot as plt

def cramers_v(x, y):
    confusion_matrix = pd.crosstab(x, y)
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    phi2 = chi2 / n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))

cat_cols = df.select_dtypes(include=['object', 'category']).columns

# Create a matrix for Cramér’s V
cramers_results = pd.DataFrame(index=cat_cols, columns=cat_cols)

for col1 in cat_cols:
    for col2 in cat_cols:
        if col1 == col2:
            cramers_results.loc[col1, col2] = 1.0
        else:
            cramers_results.loc[col1, col2] = cramers_v(df[col1], df[col2])

# Convert to float
cramers_results = cramers_results.astype(float)

# Plot heatmap
plt.figure(figsize=(10,8))
sns.heatmap(cramers_results, annot=True, cmap="coolwarm", fmt='.2f')
plt.title("Cramér’s V Correlation Between Categorical Variables")
plt.show()

"""---
**DATA VISUALIZATION**
---

---


**Relation Between Numerical Features and The Target (The Price)**
---
"""

print(numeric_cols)

import plotly.express as px

# Define the categorical columns that are numeric in nature
num_categorical_cols = [ 'Bathrooms', 'Bedrooms', 'House Age','Year Built']
for col in num_categorical_cols:
    avg_price = df.groupby(col)['List Price'].mean().reset_index()

    # Sort by price descending
    avg_price = avg_price.sort_values('List Price', ascending=False)

    fig = px.bar(avg_price, x=col, y='List Price',
                 title=f'Average List Price by {col}',
                 labels={col: col, 'List Price': 'Average List Price'},
                 color='List Price', color_continuous_scale='Viridis')


    fig.update_layout(
        xaxis_title=col,
        yaxis_title='Average List Price',
        title=f'Average List Price by {col}',
        template='plotly_white',
        xaxis_tickangle=-45
    )

    fig.show()

"""**OBSERVATION**


---

*  **Bathrooms – 5**
→ Houses with 5 bathrooms have an average list price of $1,000,247, which is the highest among all bathroom counts.


*  **Bedrooms – 6**
→ Properties with 6 bedrooms have the highest average price of $846,249.

*  **House Age – 92 years**
→ Homes that are 92 years old (built around 1933) also have the same high average price as seen in Year Built.

*   **Year Built – 1933**
→ Houses built in 1933 have the highest average list price of $1,238,140.








"""

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

numeric_cols = [col for col in numeric_cols if col not in ['City_freq', 'List Price']]

for col in numeric_cols:
    fig = px.histogram(df, x=col, color='List Price',
                       title=f'{col} Distribution Colored by Price',
                       template='plotly_white')
    fig.show()

import statsmodels.api as sm
from statsmodels.formula.api import ols

numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

#Anova for num features
for col in numeric_cols:
    model = ols(f'Q("List Price") ~ Q("{col}")', data=df).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)
    print(f'\nANOVA for {col}')
    print(anova_table)

# Calculate the correlation between numeric features and 'List Price'
correlation_matrix = df[numeric_cols].corr()
print("\nCorrelation Matrix:")
# Sorting the correlation values for 'List Price' in descending order
print(correlation_matrix['List Price'].sort_values(ascending=False))

import plotly.figure_factory as ff

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
numeric_cols = [col for col in numeric_cols if col not in ['City_freq', 'List Price']]

# The 'numeric_cols' is already a list, so 'tolist()' is not needed
if 'List Price' not in numeric_cols:
    numeric_cols = numeric_cols + ['List Price']

# Compute correlation matrix using only the existing columns in df
correlation_matrix = df[[col for col in numeric_cols if col in df.columns]].corr()

# Create the heatmap
fig = ff.create_annotated_heatmap(
    z=correlation_matrix.values,
    x=list(correlation_matrix.columns),
    y=list(correlation_matrix.index),
    colorscale='Viridis'
)

fig.update_layout(title='Correlation Heatmap Including List Price')
fig.show()

"""---


**OBSERVATION**

---
--**Bathrooms and Bedrooms** show the strongest positive correlations with List Price.

--Square Footage and Year Built also show positive correlations, but they are weaker.

--House Age has a negative correlation, suggesting that older houses tend to be priced lower, though the effect is weak.

---


**Relation Between Categorical Features and The Target (The Price)**
---
"""

import plotly.express as px

fig = px.box(df, x='House Type', y='List Price', title='Price by House Type')
fig.show()

fig = px.box(df, x='City', y='List Price', title='Price by City')
fig.show()

fig = px.box(df, x='State', y='List Price', title='Price by State')
fig.show()

fig = px.box(df, x='House Size', y='List Price', title='Price by House Size')
fig.show()

"""**Observation**

House Size: Larger houses clearly tend to have higher prices than medium and small houses

State: Some states consistently have more expensive properties

City: There's a clear variation in prices between cities. Some cities have much higher median prices, which may reflect location demand or neighborhood value

"""

import plotly.express as px

categorical_cols = ['House Size', 'House Type', 'State','City']

for col in categorical_cols:
    # Calculate the average 'List Price' for each category in the categorical column
    avg_price = df.groupby(col)['List Price'].mean().reset_index()

    avg_price = avg_price.sort_values('List Price', ascending=False)

    # Create an interactive bar plot using Plotly
    fig = px.bar(avg_price, x=col, y='List Price',
                 title=f'Average List Price by {col}',
                 labels={col: col, 'List Price': 'Average List Price'},
                 color='List Price', color_continuous_scale='Viridis')

    fig.update_layout(
        xaxis_title=col,
        yaxis_title='Average List Price',
        title=f'Average List Price by {col}',
        template='plotly_white',
        xaxis_tickangle=-45
    )

    fig.show()

"""**Observation**

--**State**: North VA and DC have the highest prices

--**House Size**: larger house sizes show a wider price range and more outliers


--**House Type**: Single-family home has a great impact on the price

--**City**: North Bethesda is the highest priced city


"""

from scipy.stats import f_oneway
import pandas as pd

# List of categorical features to analyze
cat_features = ['House Type', 'City', 'State', 'House Size']

# Create an empty list to store results
anova_results = []

# Loop through each categorical feature
for feature in cat_features:
    groups = [df[df[feature] == cat]['List Price'] for cat in df[feature].unique()]
    anova = f_oneway(*groups)
    anova_results.append({
        'Feature': feature,
        'F-statistic': anova.statistic,
        'p-value': anova.pvalue
    })

anova_df = pd.DataFrame(anova_results)
print(anova_df)

import plotly.express as px
fig = px.bar(
    anova_df,
    x='Feature',
    y='F-statistic',
    color='p-value',
    color_continuous_scale='RdBu_r',
    title='ANOVA F-statistic by Categorical Feature',
    text='p-value'
)

fig.update_layout(
    xaxis_title='Categorical Feature',
    yaxis_title='F-statistic',
    coloraxis_colorbar=dict(title='p-value'),
    template='plotly_white'
)

fig.show()

"""

---


**OBSERVATION**


---
--House Size has the strongest influence on List Price,

 followed by State, then City.

--This means property size impacts price the most, while geographic location also plays a significant role.


"""

x_test_scaled_df

"""

---


**BUILD MODEL**
---

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xg
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split

models={
    'Linear Regression':LinearRegression(),
    'Random Forest':RandomForestRegressor(),
    'xgboost':xg.XGBRegressor()
}
def train_and_evaluate_model(models, x_train, y_train, x_test, y_test):
    models_scores=[]
    model_scores={}
    for name,model in models.items():
      model.fit(x_train, y_train)
      y_pred = model.predict(x_test)
      mse = mean_squared_error(y_test, y_pred)
      r2 = r2_score(y_test, y_pred)
      mae = mean_absolute_error(y_test, y_pred)
      models_scores.append({
          'model_name':name,
          'mse':mse,
          'r2':r2,
          'mae':mae
      })
    return pd.DataFrame(models_scores,columns=['model_name','mse','r2','mae'])

# Define the pipeline with the preprocessor step
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())])

# Fit the pipeline to your training data
pipeline.fit(x_train, y_train)

# Predict on the test data
y_pred = pipeline.predict(x_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
print(f"Mean Absolute Error: {mae}")

from xgboost import XGBRegressor


# Get the correct numeric and categorical column names from x_train
numeric_cols = x_train.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = x_train.select_dtypes(include='object').columns

# Define preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)
    ]
)

# Define pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor())
])

# Apply log transform to y
y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

# Fit pipeline on training data
pipeline.fit(x_train, y_train_log)

# Predict on test data
y_pred_log = pipeline.predict(x_test)  # Use x_test, not x_test_scaled_df

# Inverse transform predictions
y_pred = np.expm1(y_pred_log)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
print(f"Mean Absolute Error: {mae}")

train_and_evaluate_model(models,x_train_scaled_df,y_train,x_test_scaled_df,y_test)

data = {
    'model_name': ['Linear Regression', 'Random Forest', 'XGBoost'],
    'MSE': [5.788547e+11, 2.475131e+11, 2.112614e+11],
    'R2': [0.075236, 0.604579, 0.662494],
    'MAE': [399720.712278, 204674.669047, 225574.006761]
}
results_df = pd.DataFrame(data)

fig, axs = plt.subplots(1, 3, figsize=(18, 5))
fig.suptitle('Model Performance Comparison', fontsize=16)

# Plot Mean Squared Error
axs[0].bar(results_df['model_name'], results_df['MSE'], color='skyblue')
axs[0].set_title('Mean Squared Error (MSE)')
axs[0].ticklabel_format(style='sci', axis='y', scilimits=(0, 0))
axs[0].set_ylabel('MSE')

# Plot R² Score
axs[1].bar(results_df['model_name'], results_df['R2'], color='lightgreen')
axs[1].set_title('R² Score')
axs[1].set_ylabel('R²')

# Plot Mean Absolute Error
axs[2].bar(results_df['model_name'], results_df['MAE'], color='salmon')
axs[2].set_title('Mean Absolute Error (MAE)')
axs[2].set_ylabel('MAE')

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""**OBSERVATION**
---

--XGBoost: Achieved the best overall performance with the lowest MSE (2.11×10¹¹) and the highest R² (0.662). This indicates that XGBoost is the best at explaining the variance in the data and making accurate predictions.

--Random Forest: Has the lowest MAE (204,674), which means it made slightly smaller average errors than the others. However, its MSE (2.47×10¹¹) and R² (0.605) were slightly worse than XGBoost.

--Linear Regression: Performed the worst with the highest MSE (5.79×10¹¹) and MAE (399,720), and a very low R² (0.075). This shows that Linear Regression struggled to capture the underlying patterns in the data.



 Conclusion: Based on the evaluation metrics, XGBoost is the best-performing model overall, offering the best balance between error reduction and explanatory power.

"""

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# Instead of LabelEncoder, use KBinsDiscretizer for continuous target
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder


# ... (Your existing code for data loading and preprocessing) ...

# Define the number of bins for discretization
n_bins = 5  # Adjust this based on your needs

# Create a KBinsDiscretizer instance
discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')

# Fit the discretizer on the training data and transform both train and test targets
y_train_encoded = discretizer.fit_transform(y_train.values.reshape(-1, 1)).astype(int).ravel()
y_test_encoded = discretizer.transform(y_test.values.reshape(-1, 1)).astype(int).ravel()

models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Define your preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)
    ]
)

# Fit and evaluate
for name, model in models.items():
    # Create a pipeline
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])

    # Fit the pipeline
    pipeline.fit(x_train, y_train_encoded)

    # Predict
    y_pred = pipeline.predict(x_test)

    print(f'--- {name} ---')
    print(f'Accuracy: {accuracy_score(y_test_encoded, y_pred):.4f}')
    print('\nClassification Report:\n', classification_report(y_test_encoded, y_pred))
    print('Confusion Matrix:\n', confusion_matrix(y_test_encoded, y_pred))
    print('\n')

from xgboost import XGBRegressor, plot_importance
import matplotlib.pyplot as plt


model = XGBRegressor()
model.fit(x_train_scaled_df, y_train)

importance = model.feature_importances_
for col, score in zip(x_train.columns, importance):
    print(f"{col}: {score:.4f}")


plot_importance(model)
plt.show()

from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(score_func=f_regression, k=10)
X_new = selector.fit_transform(x_train_scaled, y_train)

selected_features = x_train.columns[selector.get_support()]
print("Selected Features:", selected_features.tolist())

from sklearn.feature_selection import RFE
from xgboost import XGBRegressor

model = XGBRegressor()
rfe = RFE(estimator=model, n_features_to_select=10)
rfe.fit(x_train_scaled_df, y_train)

selected_features = x_train_scaled_df.columns[rfe.support_]
print("Selected Features:", selected_features.tolist())

import shap

model2 = XGBRegressor()
model2.fit(x_train_scaled_df, y_train)

explainer = shap.Explainer(model2)
shap_values = explainer(x_train_scaled_df)

shap.plots.beeswarm(shap_values)

model2=XGBRegressor()
model2.fit(x_train_scaled_df,y_train)

r2_score(y_test,model2.predict(x_test_scaled_df))

def_pred=model2.predict(x_test_scaled)

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor

param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

xgb = XGBRegressor()
random_search = RandomizedSearchCV(xgb, param_grid, n_iter=20, cv=5, scoring='r2', verbose=2,random_state=42)
random_search.fit(x_train_scaled_df, y_train)

new_model=random_search.best_estimator_

r2_score(y_test,new_model.predict(x_test_scaled_df))

r2_score(y_test,new_model.predict(x_test_scaled_df))

from sklearn.model_selection import GridSearchCV,cross_val_score
gs=GridSearchCV(new_model,param_grid,cv=5,scoring='r2')
gs.fit(x_train_scaled_df,y_train)

model2=gs.best_estimator_

r2_score(y_test,model2.predict(x_test_scaled_df))

import matplotlib.pyplot as plt
from sklearn.metrics import r2_score

# Predict using model2
y_pred = model2.predict(x_test_scaled_df)

# Calculate R² score
r2 = r2_score(y_test, y_pred)

# Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5, color='teal')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.title(f"Actual vs Predicted Values\nR² Score: {r2:.3f}")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.grid(True)
plt.tight_layout()
plt.show()

#r2_score(y_test_log,model2.predict(x_test_scaled_df))

#cross_val_score(model2,x_train_scaled_df,y_train,cv=5,scoring='neg_mean_absolute_error')

#r2_score(y_test,model2.predict(x_test_scaled_df))